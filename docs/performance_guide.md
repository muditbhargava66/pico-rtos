# Pico-RTOS Performance Guide

This guide provides comprehensive information on optimizing performance with Pico-RTOS across all versions.

**Current Version**: v0.3.0 "Advanced Synchronization & Multi-Core"

## ðŸŽ¯ Performance Overview

### Performance Goals
- **Real-Time Determinism**: Predictable timing behavior
- **Low Latency**: Minimal interrupt and context switch overhead
- **High Throughput**: Maximum task execution efficiency
- **Memory Efficiency**: Optimal RAM and flash usage
- **Multi-Core Scaling**: Effective dual-core utilization *(v0.3.0)*

### Key Metrics
- **Context Switch Time**: 50-100 CPU cycles (ARM Cortex-M0+)
- **Interrupt Latency**: <10 cycles to ISR entry
- **Memory Overhead**: ~2KB base system + per-task overhead
- **Multi-Core Efficiency**: 80-95% scaling on RP2040 *(v0.3.0)*

---

## âš¡ Core Performance Optimization

### Context Switching Performance

#### Optimized Context Switch
Pico-RTOS uses hand-optimized assembly for context switching:

```c
// Typical context switch timing
// ARM Cortex-M0+ @ 125MHz:
// - Save context: ~25 cycles
// - Scheduler decision: ~15 cycles  
// - Restore context: ~25 cycles
// Total: ~65 cycles (0.52 microseconds)
```

#### Minimizing Context Switches
```c
// Avoid unnecessary yields
void efficient_task(void *param) {
    while (1) {
        // Do substantial work before yielding
        process_batch_of_work();
        
        // Only yield when necessary
        pico_rtos_task_delay(10);  // Better than frequent yields
    }
}

// Avoid this pattern
void inefficient_task(void *param) {
    while (1) {
        do_tiny_work();
        pico_rtos_task_yield();  // Excessive context switching
    }
}
```

### Interrupt Performance

#### Fast Interrupt Handling
```c
// Keep ISRs short and fast
void __isr gpio_interrupt_handler(void) {
    // Clear interrupt quickly
    gpio_acknowledge_irq(GPIO_PIN, GPIO_IRQ_EDGE_RISE);
    
    // Defer processing to task
    pico_rtos_semaphore_give_from_isr(&processing_semaphore);
    
    // Don't do heavy work in ISR
}

// Process in task context
void processing_task(void *param) {
    while (1) {
        pico_rtos_semaphore_take(&processing_semaphore, PICO_RTOS_WAIT_FOREVER);
        
        // Heavy processing here
        process_gpio_event();
    }
}
```

### Memory Performance

#### Stack Optimization
```c
// Right-size task stacks
pico_rtos_task_create(&task, "Task", task_func, NULL, 
                     512,   // Start small, measure actual usage
                     10);

// Monitor stack usage
pico_rtos_task_info_t info;
pico_rtos_debug_get_task_info(&task, &info);
printf("Stack used: %lu/%lu bytes (%.1f%%)\n", 
       info.stack_used, info.stack_size,
       (float)info.stack_used / info.stack_size * 100);
```

#### Memory Pool Usage *(v0.3.0)*
```c
// Use memory pools for frequent allocations
pico_rtos_memory_pool_t message_pool;
uint8_t pool_buffer[32 * 64];  // 32 blocks of 64 bytes

void init_memory_pool() {
    pico_rtos_memory_pool_init(&message_pool, pool_buffer, 64, 32);
}

// O(1) allocation vs malloc overhead
void *fast_alloc() {
    return pico_rtos_memory_pool_alloc(&message_pool, 100);
}

void fast_free(void *ptr) {
    pico_rtos_memory_pool_free(&message_pool, ptr);
}
```

---

## ðŸš€ Multi-Core Performance *(v0.3.0)*

### SMP Scheduler Optimization

#### Enabling Multi-Core
```c
int main() {
    pico_rtos_init();
    
    // Enable SMP for dual-core performance
    if (!pico_rtos_smp_init()) {
        printf("SMP initialization failed\n");
        return -1;
    }
    
    // Tasks will automatically load balance
    create_application_tasks();
    
    pico_rtos_start();
    return 0;
}
```

#### Load Balancing Configuration
```cmake
# Optimize load balancing
set(PICO_RTOS_ENABLE_LOAD_BALANCING ON)
set(PICO_RTOS_LOAD_BALANCE_THRESHOLD 75)  # Trigger at 75% CPU usage

# Enable core affinity for critical tasks
set(PICO_RTOS_ENABLE_CORE_AFFINITY ON)
```

### Core Affinity Strategies

#### Critical Task Isolation
```c
// Isolate time-critical tasks to dedicated core
pico_rtos_task_t critical_task;
pico_rtos_task_create(&critical_task, "Critical", critical_func, NULL, 1024, 25);

// Pin to core 0 for consistent timing
pico_rtos_task_set_core_affinity(&critical_task, 0x01);  // Core 0 only

// Background tasks can use both cores
pico_rtos_task_t background_task;
pico_rtos_task_create(&background_task, "Background", bg_func, NULL, 1024, 5);
// No affinity set - can run on either core
```

#### Workload Distribution
```c
// Distribute complementary workloads
void setup_workload_distribution() {
    // Core 0: Real-time processing
    pico_rtos_task_t rt_task;
    pico_rtos_task_create(&rt_task, "RealTime", rt_func, NULL, 1024, 20);
    pico_rtos_task_set_core_affinity(&rt_task, 0x01);
    
    // Core 1: Background processing
    pico_rtos_task_t bg_task;
    pico_rtos_task_create(&bg_task, "Background", bg_func, NULL, 1024, 10);
    pico_rtos_task_set_core_affinity(&bg_task, 0x02);
}
```

### Inter-Core Communication

#### High-Performance IPC
```c
// Use IPC channels for efficient inter-core communication
pico_rtos_ipc_channel_t ipc_channel;
uint8_t ipc_buffer[1024];

void init_ipc() {
    pico_rtos_ipc_channel_init(&ipc_channel, ipc_buffer, sizeof(ipc_buffer));
}

// Core 0: Producer
void producer_task(void *param) {
    while (1) {
        data_packet_t packet = generate_data();
        
        // Fast inter-core send
        pico_rtos_ipc_send(&ipc_channel, &packet, sizeof(packet), 100);
    }
}

// Core 1: Consumer
void consumer_task(void *param) {
    while (1) {
        data_packet_t packet;
        
        // Fast inter-core receive
        if (pico_rtos_ipc_receive(&ipc_channel, &packet, sizeof(packet), 1000)) {
            process_data(&packet);
        }
    }
}
```

---

## ðŸ”„ Synchronization Performance

### Event Groups vs Traditional Methods *(v0.3.0)*

#### Performance Comparison
```c
// Traditional flag checking (inefficient)
volatile uint32_t flags = 0;
pico_rtos_mutex_t flag_mutex;

void wait_traditional() {
    while (1) {
        pico_rtos_mutex_lock(&flag_mutex, PICO_RTOS_WAIT_FOREVER);
        if (flags & REQUIRED_FLAGS) {
            flags &= ~REQUIRED_FLAGS;
            pico_rtos_mutex_unlock(&flag_mutex);
            break;
        }
        pico_rtos_mutex_unlock(&flag_mutex);
        pico_rtos_task_delay(1);  // Polling overhead
    }
}

// Event groups (efficient)
pico_rtos_event_group_t events;

void wait_efficient() {
    // Blocks until events are set, no polling
    uint32_t result = pico_rtos_event_group_wait_bits(
        &events, REQUIRED_FLAGS, false, true, PICO_RTOS_WAIT_FOREVER);
}
```

### Stream Buffer Performance *(v0.3.0)*

#### Zero-Copy Optimization
```c
// Configure zero-copy threshold
#define ZERO_COPY_THRESHOLD 128

pico_rtos_stream_buffer_t stream;
uint8_t stream_buffer[4096];

void init_stream() {
    pico_rtos_stream_buffer_init(&stream, stream_buffer, sizeof(stream_buffer));
}

// Large messages use zero-copy automatically
void send_large_data(const void *data, uint32_t size) {
    if (size >= ZERO_COPY_THRESHOLD) {
        // Zero-copy path automatically used
        pico_rtos_stream_buffer_send(&stream, data, size, 1000);
    }
}
```

### Queue vs Stream Buffer Performance

#### When to Use Each
```c
// Use queues for fixed-size messages
typedef struct {
    uint32_t sensor_id;
    float value;
    uint32_t timestamp;
} sensor_reading_t;

pico_rtos_queue_t sensor_queue;
sensor_reading_t queue_buffer[16];

// Efficient for fixed-size data
pico_rtos_queue_init(&sensor_queue, queue_buffer, 
                    sizeof(sensor_reading_t), 16);

// Use stream buffers for variable-size messages
pico_rtos_stream_buffer_t data_stream;
uint8_t stream_buffer[2048];

// Efficient for variable-size data
pico_rtos_stream_buffer_init(&data_stream, stream_buffer, 
                            sizeof(stream_buffer));
```

---

## ðŸ“Š Performance Monitoring

### Built-in Profiling *(v0.3.0)*

#### Execution Time Profiling
```c
void profile_critical_function() {
    // Start profiling
    pico_rtos_profiler_start();
    
    // Your critical code
    critical_processing_function();
    
    // Stop profiling
    pico_rtos_profiler_stop();
    
    // Get results
    pico_rtos_profiler_stats_t stats;
    if (pico_rtos_profiler_get_stats(&stats)) {
        printf("Execution time: %lu us (min: %lu, max: %lu)\n",
               stats.avg_execution_time_us,
               stats.min_execution_time_us,
               stats.max_execution_time_us);
    }
}
```

#### System Performance Monitoring
```c
void monitor_system_performance() {
    pico_rtos_system_stats_t stats;
    
    if (pico_rtos_get_system_stats(&stats)) {
        printf("System Performance:\n");
        printf("  CPU Usage: %lu%%\n", stats.cpu_usage_percent);
        printf("  Memory Used: %lu/%lu bytes\n", 
               stats.memory_used, stats.memory_total);
        printf("  Context Switches: %lu/sec\n", stats.context_switches_per_sec);
        
        #ifdef PICO_RTOS_ENABLE_MULTI_CORE
        printf("  Core 0 Usage: %lu%%\n", stats.cpu_usage_core0);
        printf("  Core 1 Usage: %lu%%\n", stats.cpu_usage_core1);
        printf("  Load Balance Events: %lu\n", stats.load_balance_events);
        #endif
    }
}
```

### Task Performance Analysis

#### Task Inspection *(v0.3.0)*
```c
void analyze_task_performance(pico_rtos_task_t *task) {
    pico_rtos_task_info_t info;
    
    if (pico_rtos_debug_get_task_info(task, &info)) {
        printf("Task: %s\n", info.name);
        printf("  State: %s\n", task_state_to_string(info.state));
        printf("  Priority: %lu\n", info.priority);
        printf("  Stack: %lu/%lu bytes (%.1f%% used)\n",
               info.stack_used, info.stack_size,
               (float)info.stack_used / info.stack_size * 100);
        printf("  CPU Time: %lu ms\n", info.cpu_time_ms);
        printf("  Context Switches: %lu\n", info.context_switches);
        
        #ifdef PICO_RTOS_ENABLE_MULTI_CORE
        printf("  Core Affinity: 0x%02X\n", info.core_affinity);
        printf("  Current Core: %lu\n", info.current_core);
        #endif
    }
}
```

---

## ðŸ”§ Configuration Optimization

### Memory Configuration

#### Optimal Memory Settings
```cmake
# Balance memory usage vs performance
set(PICO_RTOS_MAX_TASKS 16)                    # Adjust to actual needs
set(PICO_RTOS_DEFAULT_TASK_STACK_SIZE 1024)    # Start conservative
set(PICO_RTOS_IDLE_TASK_STACK_SIZE 256)        # Minimal for idle

# v0.3.0 feature limits
set(PICO_RTOS_MAX_EVENT_GROUPS 8)              # Adjust to usage
set(PICO_RTOS_MAX_STREAM_BUFFERS 4)            # Adjust to usage
set(PICO_RTOS_MAX_MEMORY_POOLS 4)              # Adjust to usage

# Debugging overhead (disable in production)
set(PICO_RTOS_ENABLE_SYSTEM_TRACING OFF)       # Saves ~512 bytes
set(PICO_RTOS_ENABLE_EXECUTION_PROFILING OFF)  # Saves ~256 bytes
```

### Timing Configuration

#### Tick Rate Optimization
```cmake
# Higher tick rate = better time resolution, more overhead
set(PICO_RTOS_TICK_RATE_HZ 1000)  # 1ms resolution (recommended)

# For high-precision timing requirements
set(PICO_RTOS_TICK_RATE_HZ 2000)  # 0.5ms resolution

# For power-sensitive applications
set(PICO_RTOS_TICK_RATE_HZ 500)   # 2ms resolution
```

### Feature-Specific Optimization

#### Multi-Core Settings *(v0.3.0)*
```cmake
# Optimize load balancing
set(PICO_RTOS_LOAD_BALANCE_THRESHOLD 75)       # Trigger at 75% CPU
set(PICO_RTOS_ENABLE_CORE_AFFINITY ON)         # Allow task pinning

# IPC optimization
set(PICO_RTOS_IPC_CHANNEL_BUFFER_SIZE 512)     # Adjust to message sizes
```

---

## ðŸ“ˆ Performance Benchmarks

### Context Switch Benchmarks

#### Measured Performance (RP2040 @ 125MHz)
```
Configuration          | Context Switch Time | Memory Overhead
--------------------- | ------------------- | ---------------
Minimal (v0.3.0)     | 52 cycles (0.42Î¼s) | 1.8KB + stacks
Default (v0.3.0)      | 58 cycles (0.46Î¼s) | 2.4KB + stacks  
Full (v0.3.0)         | 65 cycles (0.52Î¼s) | 3.2KB + stacks
Multi-Core (v0.3.0)   | 68 cycles (0.54Î¼s) | 3.6KB + stacks
```

### Synchronization Benchmarks

#### Operation Performance (RP2040 @ 125MHz)
```
Operation                    | Time (cycles) | Time (Î¼s)
---------------------------- | ------------- | ---------
Mutex Lock/Unlock           | 28            | 0.22
Semaphore Take/Give          | 24            | 0.19
Queue Send/Receive           | 32            | 0.26
Event Group Set/Wait (v0.3.0)| 26            | 0.21
Stream Buffer Send (v0.3.0)  | 35            | 0.28
Memory Pool Alloc (v0.3.0)   | 18            | 0.14
```

### Multi-Core Scaling *(v0.3.0)*

#### Parallel Processing Efficiency
```
Workload Type           | Single Core | Dual Core | Efficiency
----------------------- | ----------- | --------- | ----------
CPU-Intensive Tasks     | 100%        | 185%      | 92.5%
I/O-Bound Tasks         | 100%        | 165%      | 82.5%
Mixed Workload          | 100%        | 175%      | 87.5%
```

---

## ðŸŽ¯ Performance Best Practices

### General Guidelines

#### Task Design
1. **Right-Size Stacks**: Start small, measure actual usage
2. **Minimize Blocking**: Use timeouts appropriately
3. **Batch Operations**: Group related work together
4. **Avoid Busy Waiting**: Use proper synchronization

#### Memory Management
1. **Use Memory Pools**: For frequent allocations *(v0.3.0)*
2. **Monitor Usage**: Track memory consumption
3. **Avoid Fragmentation**: Use consistent allocation patterns
4. **Stack Monitoring**: Check for overflow risks

### Multi-Core Guidelines *(v0.3.0)*

#### Task Distribution
1. **Identify Parallelizable Work**: Look for independent tasks
2. **Minimize Shared State**: Reduce synchronization overhead
3. **Use Core Affinity Wisely**: Pin critical tasks when needed
4. **Balance Workloads**: Avoid core imbalance

#### Synchronization Strategy
1. **Choose Right Primitive**: Event groups vs queues vs mutexes
2. **Minimize Lock Contention**: Keep critical sections short
3. **Use Lock-Free Patterns**: When possible *(advanced)*
4. **Profile Synchronization**: Identify bottlenecks

### Debugging Performance Issues

#### Common Performance Problems
```c
// Problem: Excessive context switching
void bad_task(void *param) {
    while (1) {
        do_small_work();
        pico_rtos_task_yield();  // Too frequent
    }
}

// Solution: Batch work
void good_task(void *param) {
    while (1) {
        for (int i = 0; i < 10; i++) {
            do_small_work();
        }
        pico_rtos_task_delay(1);  // Appropriate yielding
    }
}

// Problem: Polling instead of blocking
void bad_polling(void *param) {
    while (1) {
        if (check_condition()) {
            handle_condition();
        }
        pico_rtos_task_delay(1);  // Wasteful polling
    }
}

// Solution: Use proper synchronization
void good_blocking(void *param) {
    while (1) {
        pico_rtos_semaphore_take(&condition_semaphore, PICO_RTOS_WAIT_FOREVER);
        handle_condition();  // Only runs when needed
    }
}
```

---

## ðŸ” Performance Troubleshooting

### Diagnostic Tools

#### Performance Analysis Checklist
1. **Profile Critical Paths**: Use execution profiling *(v0.3.0)*
2. **Monitor System Stats**: Check CPU and memory usage
3. **Analyze Task Behavior**: Use task inspection *(v0.3.0)*
4. **Check Synchronization**: Look for contention points
5. **Validate Multi-Core Usage**: Ensure load balancing *(v0.3.0)*

#### Common Issues and Solutions

**High CPU Usage**
```c
// Check for busy-wait loops
pico_rtos_system_stats_t stats;
pico_rtos_get_system_stats(&stats);
if (stats.cpu_usage_percent > 90) {
    // Investigate high-usage tasks
    analyze_all_tasks();
}
```

**Poor Multi-Core Scaling**
```c
// Check load balance effectiveness
if (stats.cpu_usage_core0 > 90 && stats.cpu_usage_core1 < 50) {
    // Consider adjusting core affinity or load balance threshold
    adjust_core_affinity();
}
```

**Memory Issues**
```c
// Monitor memory pool efficiency
pico_rtos_memory_pool_stats_t pool_stats;
pico_rtos_memory_pool_get_stats(&pool, &pool_stats);
if (pool_stats.fragmentation_percent > 25) {
    // Consider pool size adjustment
}
```

---

## ðŸ“‹ Performance Optimization Checklist

### Pre-Optimization
- [ ] Profile current performance baseline
- [ ] Identify performance-critical code paths
- [ ] Measure memory usage patterns
- [ ] Document current system behavior

### Core Optimization
- [ ] Optimize task stack sizes
- [ ] Minimize context switch frequency
- [ ] Use appropriate synchronization primitives
- [ ] Implement efficient interrupt handling

### v0.3.0 Features
- [ ] Evaluate event groups for complex synchronization
- [ ] Consider stream buffers for variable-length data
- [ ] Use memory pools for frequent allocations
- [ ] Enable multi-core support where beneficial

### Multi-Core Optimization *(v0.3.0)*
- [ ] Identify parallelizable workloads
- [ ] Configure load balancing appropriately
- [ ] Set core affinity for critical tasks
- [ ] Optimize inter-core communication

### Validation
- [ ] Measure performance improvements
- [ ] Validate real-time requirements still met
- [ ] Test under various load conditions
- [ ] Monitor long-term stability

---

**Performance Guide Version**: v0.3.0  
**Last Updated**: July 25, 2025  
**Benchmark Platform**: RP2040 @ 125MHz